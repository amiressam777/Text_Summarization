{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "556dd130",
        "outputId": "2e1d742e-fad7-4cdd-ea72-1ed0354602f4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74452ce4",
        "outputId": "e66a88cd-c0ab-4b8c-af27-c1bb557e135d"
      },
      "source": [
        "text = \"This application aims to provide an efficient and user-friendly solution to [problem/topic]\"\n",
        "\n",
        "\n",
        "#preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# إعداد الـ Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text.split()\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # إزالة الروابط\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "    # إزالة الإشارات والهاشتاجات\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # إزالة الأرقام والرموز الخاصة\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # إزالة الكلمات الشائعة\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # تطبيق Stemming على الكلمات\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered]\n",
        "\n",
        "    # إزالة الكلمات القصيرة جدًا\n",
        "    clean_tokens = [word for word in stemmed_words if len(word) > 2]\n",
        "\n",
        "    # تجميع الكلمات من جديد\n",
        "    clean_text = ' '.join(clean_tokens)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# تحميل التوكنيزر والموديل\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "\n",
        "inputs = tokenizer([preprocess_text(text)], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "# التلخيص\n",
        "summary_ids = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=150,\n",
        "    min_length=40,\n",
        "    length_penalty=2.0,\n",
        "    num_beams=4,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# استخراج النص الملخص\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Summary:\", summary)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: applic aim provid effici userfriendli solut problemtop.applic Aim: To make the world a better place. Goal: To improve the quality of life for all people.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swxvaDWr8kdQ"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}